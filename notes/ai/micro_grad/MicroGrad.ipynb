{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5c686c-107a-4f76-8c14-953abe0d11ec",
   "metadata": {},
   "source": [
    "# Micrograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5850e2-54bc-4727-bc4e-da0c954384ef",
   "metadata": {},
   "source": [
    "Micrograd is a small library made by Andrej Karpathy. This is a reimplementation made by me.\n",
    "I wanted to have a version of Micrograd with a ton of documentation after watching the amazing\n",
    "video made possible by Adrej that [you can view here](https://www.youtube.com/watch?v=VMj-3S1tku0).\n",
    "\n",
    "The code is licensed under the MIT license and you can use it for whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "73680d3c-811e-413f-9654-e3ea2e22739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525a8d7-135b-46a7-9666-29514413a3d3",
   "metadata": {},
   "source": [
    "# We have to create a datastructure (The Engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce9fa6-6aee-468b-971c-fcaf6452cd40",
   "metadata": {},
   "source": [
    "Creating a new datastructure so our life is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f0329db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, value):\n",
    "        # Create a constructor for the datastructure\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bb6b8",
   "metadata": {},
   "source": [
    "We are going to add a bit more. We are going to make it able to add between the Tensor type and a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87083d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, value):\n",
    "        # Create a constructor for the datastructure\n",
    "        self.value = value\n",
    "    def __add__(self, other): \n",
    "        # self + other\n",
    "        return Tensor(self.value + other.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd59cc8",
   "metadata": {},
   "source": [
    "That seems like enough for now. Now we should check what is a gradient.\n",
    "\n",
    "The gradient is a small step forward in the direction of our derivative.\n",
    "\n",
    "But we should have a way to store this information. So we should add this to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "192116cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, value):\n",
    "        # Create a constructor for the datastructure\n",
    "        self.value = value\n",
    "        self.grad = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde67136",
   "metadata": {},
   "source": [
    "And now that we have the possiblity to have a gradient, we should look back at what calculus is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c4902",
   "metadata": {},
   "source": [
    "# Looking at you Calculus!\n",
    "\n",
    "So we need can look at the definition of a derivative\n",
    "\n",
    "$$x = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(a+\\Delta x)-f(a)}{\\Delta x}$$\n",
    "\n",
    "This did not tell me anything at first. So let's do an example.\n",
    "\n",
    "For example. If we have a value $a = 3.0$ and a value $b=0.5$ we can get their derivatives with respect to each other.\n",
    "\n",
    "So we can see the derivatives with respect to each other if we add them. We are going to name the result of this operation $c$\n",
    "\n",
    "$$c = a+b$$\n",
    "\n",
    "And now we can get the derivative with respect to $a$ and the derivative with respect to $b$.\n",
    "\n",
    "So we are going to do a _limit_. This might seem scary but what the thing above is telling us is just to add a really small amount to see if it works.\n",
    "\n",
    "So we are going to say: $\\Delta x = 0.0001$\n",
    "\n",
    "We have our $c1$ that is the value of $3.5$ and we increment the value of a by $0.0001$ so we get $c2 = 3.0001 + 0.5 = 3.5001$ and we can evaluate this value in\n",
    "the derivative:\n",
    "\n",
    "$$\\frac{c2-c1}{\\Delta x} = \\frac{ 3.5001-3.5}{0.0001} = \\frac{0.0001}{0.0001} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3943005-d0b5-42a9-968f-91c08913d587",
   "metadata": {},
   "source": [
    "Now we can do the same but for other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc0fd75-4178-4373-83a7-67a54f59ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3.0\n",
    "b = 1.5\n",
    "h = 0.0001\n",
    "\n",
    "c1 = a*b\n",
    "c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382dd97-3e26-4d1c-91ac-f396d40e6cbf",
   "metadata": {},
   "source": [
    "We can see the difference in the slope by going through the same process we did with the adition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4ac241e-f82e-4bae-aa35-278db40e6f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.500000000005386"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += h\n",
    "b = 1.5\n",
    "\n",
    "c2 = a*b\n",
    "derivative = (c2-c1)/h\n",
    "derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b88f9-f688-4933-af36-b4fe4c595e4b",
   "metadata": {},
   "source": [
    "And for the derivative of b with respect to a we can see the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd81973-501c-4175-9bdb-413aa665d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.00000000000189"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3.0\n",
    "b = 1.5\n",
    "h = 0.0001\n",
    "\n",
    "c1 = a*b\n",
    "b += h\n",
    "c2 = a*b\n",
    "\n",
    "(c2-c1)/h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a8527-b677-407c-ad27-4791a4ab4ee1",
   "metadata": {},
   "source": [
    "We can see that the values of the derivatives for $a$ and $b$ swap in the derivative. \n",
    "So we can modify our Tensor class to store the gradients of this operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e575c5-6dfb-48c5-b7c4-1a3a369db0a5",
   "metadata": {},
   "source": [
    "# The Full Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4e7590be-27b7-46c1-9a07-b8bc91cb0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "        Stores a single value.\n",
    "    \"\"\"\n",
    "    def __init__(self, value, _children=(), _operation=\"\"):\n",
    "        # Create a constructor for the datastructure\n",
    "        self.value = value\n",
    "        self.gradient = 0.0\n",
    "        \n",
    "        # Internal variables\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        # Debugging\n",
    "        self._operation = _operation\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        # self + other\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output = Tensor(self.value + other.value, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.gradient += output.gradient\n",
    "            other.gradient += output.gradient\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        # self * other\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        output = Tensor(self.value * other.value, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # we use the chain rule so that is why we multiply out.gradient * self and out.grad * other.value\n",
    "            self.gradient += other.value * output.gradient\n",
    "            other.gradient += self.value * output.gradient\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        # self ** other\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        output = Tensor(self.value ** other.value, (self, other), '^')\n",
    "        \n",
    "        def _backward():\n",
    "            self.gradient += (other*self.value ** (other-1))*output.gradient\n",
    "        \n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self):\n",
    "        # Topological order of the children in the graph\n",
    "        # Using the dfs version\n",
    "        topological_order = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(v)\n",
    "                topological_order.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.gradient = 1.0\n",
    "        \n",
    "        for v in reversed(topological_order):\n",
    "            v._backward()\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self*-1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # This function returns the value formatted when you print it in jupyterlab\n",
    "        return f\"Tensor=({self.value}, gradient={self.gradient})\"\n",
    "    \n",
    "    # Extra\n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Tensor(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward(): \n",
    "            self.gradient += (1 - t**2) * out.gradient\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96adbc56-3a65-4910-85d3-672bd4c0487a",
   "metadata": {},
   "source": [
    "# The nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5c91e2e4-7a02-4306-8bf6-cbdff0e430c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.gradient = 0\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d61edb78-6ef0-4de1-b56b-0e1b19479990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\" Class Neuron inherits from Module \"\"\"\n",
    "    \n",
    "    def __init__(self, nin):\n",
    "        # Initialize number in neuron\n",
    "        # Nin --> Number of inputs\n",
    "        self.w = [Tensor(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Tensor(0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # w * x +b\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "55e42b33-6547-4b7d-9366-402d0801a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "935bfb28-de0c-49d6-8b1f-e31e4942f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "        Multi-Layer Perceptron\n",
    "    \"\"\"\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83dd8ec-2133-4691-84ab-fb885a00ea6f",
   "metadata": {},
   "source": [
    "# An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3169d539-4115-477f-b929-88faa2bec85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor=(0.07683646136043382, gradient=0.0)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a72ad317-39a1-478c-b559-80041f8dba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097f79b-2be5-4cb6-aecd-01f8e43fb744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
